\documentclass[12pt, fullpage,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}

\newcommand{\semester}{Fall 2021}
\newcommand{\assignmentId}{0}
\newcommand{\releaseDate}{24 Aug, 2021}
\newcommand{\dueDate}{11:59pm, 3 Sep, 2021}

\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}

\title{CS 5350/6350: Machine Learining \semester}
\author{Ryan Dalby- Homework \assignmentId}
\date{Handed out: \releaseDate\\
  Due: \dueDate}

\begin{document}
\maketitle

\input{emacscomm}
\footnotesize
	\begin{itemize}
		\item You are welcome to talk to other members of the class about
		the homework. I am more concerned that you understand the
		underlying concepts. However, you should write down your own
		solution. Please keep the class collaboration policy in mind.
		
		\item Feel free to discuss the homework with the instructor or the TAs.
		
		\item Your written solutions should be brief and clear. You need to
		show your work, not just the final answer, but you do \emph{not}
		need to write it in gory detail. Your assignment should be {\bf no
			more than 10 pages}. Every extra page will cost a point.
		
		\item Handwritten solutions will not be accepted.
		
		\item The homework is due by \textbf{midnight of the due date}. Please submit
		the homework on \textbf{Canvas}.
		
		\item Some questions are marked {\bf For 6350 students}. Students
		who are registered for CS 6350 should do these questions. Of
		course, if you are registered for CS 5350, you are welcome to do
		the question too, but you will not get any credit for it.
		
	\end{itemize}



\section*{Basic Knowledge Review}
\label{sec:q1}
\begin{enumerate}
\item~[5 points] We use sets to represent events. For example, toss a fair coin $10$ times, and the event can be represented by the set of ``Heads" or ``Tails" after each tossing. Let a specific event $A$ be ``at least one head". Calculate the probability that event $A$ happens, i.e., $p(A)$.

% 1
\textit{Answer:} 

Let specific event $B$ be ``no heads". Thus the probability of ``no heads'' is the same as flipping 10 tails thus $P(B)=(0.5)^{10}$
We can represent $p(A)$ as $1-P(B)$ thus $p(A) = 1-(0.5)^{10} = 0.9990$.

%

\item~[10 points] Given two events $A$ and $B$, prove that 
\[
p(A \cup B) \le p(A) + p(B).
\]
When does the equality hold?

% 2
\textit{Answer:} 

The Addition Rule of Probability states 
\[ p(A \cup B) = p(A) + p(B) - P(A \cap B).\]
From the axioms of probability
\[ p(A) \ge 0,\ p(B) \ge 0,\ p(A \cap B) \ge 0\]
thus $p(A \cup B) = p(A) + p(B) - P(A \cap B) \le p(A) + p(B)$. This equality always holds.

%

\item~[10 points] Let $\{A_1, \ldots, A_n\}$ be a collection of events. Show that
\[
p(\cup_{i=1}^n A_i) \le \sum_{i=1}^n p(A_i).
\]
When does the equality hold? (Hint: induction)

% 3
\textit{Answer:} 

Assume $n=1$: Trivially, $p(A_1) \le p(A_1)$

Assume $n$ is true:
\[
p(\cup_{i=1}^n A_i) \le 
\sum_{i=1}^n p(A_i).
\]

Now will show $n+1$ holds:

First using the Addition Rule of Probability 
\[
p(\cup_{i=1}^{n+1} A_i) = 
p(\cup_{i=1}^{n} A_i) + p(A_{n+1}) - p(\cup_{i=1}^{n}A_i \cap A_{n+1}) 
\]
then by the axioms of probability
\[p(\cup_{i=1}^{n} A_i) \ge 0,\ p(A_{n+1}) \ge 0,\ p(\cup_{i=1}^{n}A_i \cap A_{n+1}) \ge 0\]
thus
\[
p(\cup_{i=1}^{n+1} A_i) \le 
p(\cup_{i=1}^{n} A_i) + p(A_{n+1})  
\]
then applying initial assumption that $p(\cup_{i=1}^n A_i) \le \sum_{i=1}^n p(A_i)$
\[
p(\cup_{i=1}^{n+1} A_i) \le 
p(\cup_{i=1}^{n} A_i) + p(A_{n+1}) \le 
\sum_{i=1}^n p(A_i) + p(A_{n+1}) = 
\sum_{i=1}^{n+1} p(A_i).
\]
Therefore, by induction, the initial assumption that $n$ holds is true.
%

%\item~[5 points] Given three events $A$, $B$ and $C$, show that
%\[
%p(A\cap B\cap C) = p(A|B\cap C)p(B|C)p(C)
%\]
\item~[20 points]  We use $\EE(\cdot)$ and $\VV(\cdot)$ to denote a random variable's mean (or expectation) and variance, respectively. Given two discrete random variables $X$ and $Y$, where $X \in \{0, 1\}$ and $Y \in \{0,1\}$. The joint probability $p(X,Y)$ is given in as follows:
\begin{table}[h]
        \centering
        \begin{tabular}{ccc}
        \hline\hline
         & $Y=0$ & $Y=1$ \\ \hline
         $X=0$ & $1/10$ & $2/10$ \\ \hline
         $X=1$  & $3/10$ & $4/10$ \\ \hline\hline
        \end{tabular}
        %\caption{Training data for the alien invasion problem.}\label{tb-alien-train}
\end{table}
	
\begin{enumerate}
    \item~[10 points] Calculate the following distributions and statistics. 
    \begin{enumerate}
    \item the the marginal distributions $p(X)$ and $p(Y)$

    % 4ai
    \textit{Answer:} 

    $p(X)$: 
    \[p(X=0)=3/10,\ p(X=1)=7/10\]
    $p(Y)$: 
    \[p(Y=0)=4/10 ,\ p(Y=1)=6/10.\]
    %

    \item the conditional distributions $p(X|Y)$ and $p(Y|X)$

    % 4aii
    \textit{Answer:} 

    $p(X|Y)$: 
    \[p(X=0|Y=0)=1/4,\ p(X=1|Y=0)=3/4,\]
    \[p(X=0|Y=1)=2/6,\ p(X=1|Y=1)=4/6\]

    $p(Y|X)$: 
    \[p(Y=0|X=0)=1/3,\ p(Y=1|X=0)=2/3,\]
    \[p(Y=0|X=1)=3/7,\ p(Y=1|X=1)=4/7.\]
    %

    \item $\EE(X)$, $\EE(Y)$, $\VV(X)$, $\VV(Y)$

    % 4aiii
    \textit{Answer:} 

    Note for expectation for iii. and iv.: $\EE(X) = \sum_{i=1} a_i P(X=a_i)$

    Note for variance for iii. and iv.: $\V(X) = \EE(X^2) - \EE(X)^2$

    \[\EE(X) = 0 + 7/10 = 0.7,\qquad \EE(Y) = 0 + 6/10 = 0.6,\]
    \[\VV(X) = (0 + 1^2(7/10)) - (7/10)^2 = 0.21,\qquad \VV(Y) = (0 + 1^2(6/10)) - (6/10)^2 = 0.24.\]
    %

    \item  $\EE(Y|X=0)$, $\EE(Y|X=1)$,  $\VV(Y|X=0)$, $\VV(Y|X=1)$ 

    % 4aiv
    \textit{Answer:} 

    \[\EE(Y|X=0) = 0 + 2/3 = 0.667,\qquad \EE(Y|X=1) = 0 + 4/7 = 0.571,\]
    \[\VV(Y|X=0) = (0 + 1^2(2/3)) - (2/3)^2 = 0.222,\, \VV(Y|X=1) = (0 + 1^2(4/7)) - (4/7)^2 = 0.245.\]
    %


    \item  the covariance between $X$ and $Y$

    % 4av
    \textit{Answer:} 

    Note for covariance for v.: 
    
    $Cov(X,Y) = \EE(XY) - \EE(X)\EE(Y)$ and $\EE(XY) = \sum_{i}\sum_{j}a_i b_j P(X=a_i, Y=b_j)$
   

    \[Cov(X,Y) = 0 + 0 + 0 + ((1)(1)(4/10)) - (0.7)(0.6) = -0.02.\]
    %

    \end{enumerate}
    \item~[5 points] Are $X$ and $Y$ independent? Why?

    % 4b
    \textit{Answer:} 

    No. If X and Y are independent then $Cov(X,Y) = 0$. In 4(a)v. it was determined $Cov(X,Y) = -0.02$ so X and Y are not independent.
    %


    \item~[5 points] When $X$ is not assigned a specific value, are $\EE(Y|X)$ and $\VV(Y|X)$ still constant? Why?

    % 4c
    \textit{Answer:} 

    No. If $X$ is not assigned to a specific value then the distribution is not constant since $\EE(Y|X=0) \neq \EE(Y|X=1)$ and $\VV(Y|X=0) \neq \VV(Y|X=1)$ from the results of 4(a)iv..
    %
\end{enumerate}

\item~[10 points] Assume a random variable $X$ follows a standard normal distribution, \ie $X \sim \N(X|0, 1)$. Let $Y = e^X$. Calculate the mean and variance of $Y$.
\begin{enumerate}
	\item $\EE(Y)$

  % 5a
  \textit{Answer:} 

  Using $\EE(g(X)) = \int_{-\infty}^{\infty} g(x) f(x) \, \mathrm{d}x$ where $g$ is the function described by $Y = e^X$ and 
  $f$ is the pdf of a normal function with $\mu = 0$ and $\sigma^2 = 1$

  \[
  \EE(Y) = \int_{-\infty}^{\infty} (e^x) (\frac{1}{\sqrt{2 \pi}} e^{-x^2/2}) \, \mathrm{d}x =
  \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{-x^2/2 + x} \, \mathrm{d}x.
  \]
  Then completing the square in the exponent of $e$ and evaluating the integral we get
  \[
  \EE(Y) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{\frac{-(x-1)^2}{2} + \frac{1}{2}} \, \mathrm{d}x = 
  \frac{e^{\frac{1}{2}}}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{\frac{-(x-1)^2}{2}} \, \mathrm{d}x =
  e^{\frac{1}{2}} = 1.649.
  \]
  %

	\item $\VV(Y)$

  % 5b
  \textit{Answer:} 

  Using a definition of variance $\VV(Y) = \EE(Y^2) - \EE(Y)^2$ we know $\EE(Y)^2$ from 5(a).
  Using the definitions from 5(a) where $g$ is represented now by $Y^2 = e^{2X}$ we can solve for $\EE(Y^2)$

  \[
  \EE(Y^2) = \int_{-\infty}^{\infty} (e^{2x}) (\frac{1}{\sqrt{2 \pi}} e^{-x^2/2}) \, \mathrm{d}x =
  \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{-x^2/2 + 2x} \, \mathrm{d}x.
  \]
  Then completing the square in the exponent of $e$ and evaluating the integral we get
  \[
  \EE(Y^2) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{\frac{-(x-2)^2}{2} + 2} \, \mathrm{d}x = 
  \frac{e^2}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{\frac{-(x-2)^2}{2}} \, \mathrm{d}x = e^2.
  \]
  Thus
  \[
  \VV(Y) = e^2 - (e^{\frac{1}{2}})^2 = e^2 - e = 4.671.
  \]

  %

\end{enumerate}

\item  Given two random variables $X$ and $Y$, show that 
\begin{enumerate}
  \item~[20 points] $\EE(\EE(Y|X)) = \EE(Y)$

  % 6a
  \textit{Answer:} 

  Expanding $\EE(\EE(Y|X))$
  \[
  \EE(\EE(Y|X)) = \EE(\sum_{i} a_i P(X = a_i | Y)) = \sum_j (\sum_i a_i P(X = a_i | Y = b_j)) P(Y = b_j).
  \]
  Substituting the Multiplication Rule of Probability $P(X = a_i | Y = b_j) = \frac{P(X = a_i \cap Y = b_j)}{P(Y = b_j)}$ and moving the summations 
  \[
  \EE(\EE(Y|X)) = \sum_i a_i \sum_j P(X = a_i \cap Y = b_j) = \sum_i a_i P(X = a_i) = \EE(X).
  \]

  %

  \item~[\textbf{Bonus question} 20 points]
  $\VV(Y) = \EE(\VV(Y|X)) + \VV(\EE(Y|X))$

  % 6b
  \textit{Answer:} 

  Starting with the definition of variance $\VV(Y) = \EE(Y^2) - \EE(Y)^2$. We can expand $\EE(Y^2)$ using the result from 6a) and use the rearranged definition of conditional variance $\EE(Y^2|X) = \VV(Y|X) + \EE(Y|X)^2$ as follows
  \[
  \EE(Y^2) = \EE(\EE(Y^2|X)) = \EE(\VV(Y|X) + \EE(Y|X)^2).
  \]

  Then expanding $\EE(Y)^2$ using the result from 6a
  \[
  \EE(Y)^2 = \EE(\EE(Y|X))^2.
  \]

  Combining the past two expansions with the definition of variance and using associativity 
  \[
  \VV(Y) = \EE(\VV(Y|X) + \EE(Y|X)^2) - \EE(\EE(Y|X))^2 = \EE(\VV(Y|X)) + (\EE(\EE(Y|X)^2)) - \EE(\EE(Y|X))^2).
  \]
  
  Then finally applying the definition of variance again we show the original assertion
  \[
  \VV(Y) = \EE(\VV(Y|X)) + \VV(\EE(Y|X)).
  \]
  %

\end{enumerate}
(Hints: using definition.)

%\item~[20 points]  Let us go back to the coin tossing example. Suppose we toss a coin for $n$ times, \textit{independently}. Each toss we have $\frac{1}{2}$ chance to obtain the head. Let us denote the total number of heads by $c(n)$. Derive the following statistics. You don't need to give the numerical values. You only need to provide the formula.
%\begin{enumerate}
%\item $\EE(c(1))$, $\VV(c(1))$
%\item $\EE(c(10))$, $\VV(c(10))$
%\item $\EE(c(n))$, $\VV(c(n))$
%\end{enumerate} 
%What can you conclude from comparing the expectations and variances with different choices of $n$?  

\item~[15 points] Given a logistic function, $f(\x) = 1/(1+\exp(-\a^\top \x))$ ($\x$ is a vector), derive/calculate the following gradients and Hessian matrices.  

\textit{Note: For problem 7 and 8 are assuming $\x = [x_1, x_2,..., x_n]$ and $\a = [a_1, a_2,..., a_n]$ where $n$ is the length of the vector}

\begin{enumerate}
  \item $\nabla f(\x)$

  % 7a
  \textit{Answer:} 

  \[
  \nabla f(\x) = [\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}]^{\top} =  
  [\frac{a_1 e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2}, ..., \frac{a_n e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2}]^{\top} =
  \a^\top \frac{e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2}
  \]
  %

  \item $\nabla^2 f(\x)$

  % 7b
  \textit{Answer:} 

  \[
  \nabla^2 f(\x) = 
  \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & ... & \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & ... & \frac{\partial^2 f}{\partial x_n^2}
  \end{bmatrix} =
  \begin{bmatrix}
    a_1^2 \frac{e^{-2\a^{\top} \x} - e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^3} & ... &
    a_1 a_n \frac{e^{-2\a^{\top} \x} - e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^3} \\
    \vdots & \ddots & \vdots\\
    a_n a_1 \frac{e^{-2\a^{\top} \x} - e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^3} & ... &
    a_n^2 \frac{e^{-2\a^{\top} \x} - e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^3}.
  \end{bmatrix}.
  \]
  This can also be expressed as 
  \[
  \a \a^\top \: \frac{e^{-2\a^{\top} \x} - e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^3}.
  \]
  %

  \item $\nabla f(\x)$ when $\a = [1,1,1,1,1]^\top$ and $\x = [0,0,0,0,0]^\top$

  % 7c
  \textit{Answer:} 

  With $\a = [1,1,1,1,1]^\top$ 
  \[
  \nabla f([0,0,0,0,0]^\top) = [\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]^\top.
  \]
  %

  \item $\nabla^2 f(\x)$  when $\a = [1,1,1,1,1]^\top$ and $\x = [0,0,0,0,0]^\top$

  % 7d
  \textit{Answer:} 

  With $\a = [1,1,1,1,1]^\top$ 
  \[
  \nabla^2 f([0,0,0,0,0]^\top) = 
  \begin{bmatrix}
  0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0
  \end{bmatrix} =
  \textbf{0}.
  \]
  %

\end{enumerate}
Note that $0 \le f(\x) \le 1$.

\item~[10 points] Show that $g(x) = -\log(f(\x))$ where $f(\x)$ is a logistic function defined as above, is convex. 

  % 8
  \textit{Answer:} 

  To show that $g$ is convex we will first determine $\nabla^2 g(\x)$
  \[
  \nabla^2 g(\x) = 
  \begin{bmatrix}
    \frac{\partial^2 g}{\partial x_1^2} & ... & \frac{\partial^2 g}{\partial x_1 \partial x_n}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial^2 g}{\partial x_n \partial x_1} & ... & \frac{\partial^2 g}{\partial x_n^2}
  \end{bmatrix} =
  \begin{bmatrix}
    \frac{a_1^2}{\ln(10)} \frac{e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2} & ... &
    \frac{a_1 a_n}{\ln(10)} \frac{e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2} \\
    \vdots & \ddots & \vdots\\
    \frac{a_n a_1}{\ln(10)} \frac{e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2} & ... &
    \frac{a_n^2}{\ln(10)} \frac{e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2} 
  \end{bmatrix}.
  \]
  This can also be expressed as 
  \[
  \frac{\a \a^\top}{\ln(10)} \frac{e^{-\a^{\top} \x}}{(1 + e^{-\a^{\top} \x})^2}.
  \]
  We can see that $\a \a^\top$ is positive semi-definite as well as the $e^{-\a^{\top} \x}$ function. Thus $\nabla^2 g(\x) \succeq 0$ and consequently $g$ is convex.
  %


\end{enumerate}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
