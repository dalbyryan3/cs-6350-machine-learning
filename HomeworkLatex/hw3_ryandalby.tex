\documentclass[12pt, fullpage,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{longtable}

\newcommand{\semester}{Fall 2021}
\newcommand{\assignmentId}{3}
\newcommand{\releaseDate}{19 Oct, 2021}
\newcommand{\dueDate}{11:59pm, 2 Nov, 2021}

\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}

\title{CS 5350/6350: Machine Learining \semester}
\author{Homework \assignmentId}
\date{Handed out: \releaseDate\\
	Due: \dueDate}


\title{CS 5350/6350: Machine Learning \semester}
\author{Ryan Dalby- Homework \assignmentId}
\date{Handed out: \releaseDate\\
  Due date: \dueDate}

\begin{document}
\maketitle

\input{emacscomm}
\newcommand{\Hcal}{\mathcal{H}} 
{\footnotesize
	\begin{itemize}
		\item You are welcome to talk to other members of the class about
		the homework. I am more concerned that you understand the
		underlying concepts. However, you should write down your own
		solution. Please keep the class collaboration policy in mind.
		
		\item Feel free to discuss the homework with the instructor or the TAs.
		
		\item Your written solutions should be brief and clear. You do not need to include original problem descriptions in your solutions. You need to
		show your work, not just the final answer, but you do \emph{not}
		need to write it in gory detail. Your assignment should be {\bf no
			more than 15 pages}. Every extra page will cost a point.
		
		\item Handwritten solutions will not be accepted.
		
		
		\item {\em Your code should run on the CADE machines}. \textbf{You should
		include a shell script, {\tt run.sh}, that will execute your code
		in the CADE environment. Your code should produce similar output to what you include in your report.}
		
		You are responsible for ensuring that the grader can execute the
		code using only the included script. If you are using an
		esoteric programming language, you should make sure that its
		runtime is available on CADE.
		
		\item Please do not hand in binary files! We will {\em not} grade
		binary submissions.
		
		\item The homework is due by \textbf{midnight of the due date}. Please submit
		the homework on Canvas.
		
	\end{itemize}
}


\section{Paper Problems [36 points + 15 bonus]}
\begin{enumerate}
	\item~[8 points] Suppose we have a linear classifier for $2$ dimensional features. The classification boundary, \ie  the hyperplane is $2x_1 + 3x_2 - 4 = 0$ ($x_1$ and $x_2$ are the two input features). 
	\begin{enumerate}
		
	\begin{table}[h]
		\centering
		\begin{tabular}{cc|c}
			$x_1$ & $x_2$ &  {label}\\ 
			\hline\hline
			1 & 1 & 1 \\ \hline
			1 & -1 & -1 \\ \hline
			0 & 0 & -1 \\ \hline
			-1 & 3 & 1 \\ \hline
			\end{tabular}
		\caption{Dataset 1}
	\end{table}

	\item~[4 points] Now we have a dataset in Table 1. 
	Does the hyperplane have a margin for the dataset?  If yes, what is the margin? Please use the formula we discussed in the class to compute. If no, why? (Hint: when can a hyperplane have a margin?)

	% 1a
	\textit{Answer:}

	Yes, the given data is separable by the given hyperplane and has a margin of $\frac{1}{\sqrt{13}}$ or $0.277$, it is the minimum distance of any data point to the hyperplane. 
	
	\begin{table}[h]
		\centering
		\begin{tabular}{cc|c}
			$x_1$ & $x_2$ &  {label}\\ 
			\hline\hline
			1 & 1 & 1 \\ \hline
			1 & -1 & -1 \\ \hline
			0 & 0 & -1 \\ \hline
			-1 & 3 & 1 \\ \hline
				-1 & -1 & 1 \\
		\end{tabular}
		\caption{Dataset 2}
	\end{table}

	\item~[4 points] We have a second dataset in Table 2. Does the hyperplane have a margin for the dataset? If yes, what is the margin? If no, why? 

	% 1b
	\textit{Answer:}

	No the given data is not separable by the given hyperplane.
	Plotting the given hyper plane as a function of $x_1$ as $x_2 = \frac{4}{3} - \frac{2}{3} x_1$ we see all the points of dataset 2 are separated except for the last point of $x_1=-1$ $x_2=-1$ which should have a label of $-1$ to be separable.
	(It is also possible to just calculate $\w^\top \x + b$ where $\w = [2, 3]^\top$ and $b=-4$ and notice the sign doesn't match the label, unlike the rest of the data.)
	
	\end{enumerate}
	

		\item~[8 points] Now, let us look at margins for datasets. Please review what we have discussed in the lecture and slides. A margin for a dataset is not a margin of a hyperplane!  
		\begin{enumerate}
			
			\begin{table}[h]
				\centering
				\begin{tabular}{cc|c}
					$x_1$ & $x_2$ &  {label}\\ 
					\hline\hline
					-1 & 0 & -1 \\ \hline
					0 & -1 & -1 \\ \hline
					1 & 0 & 1 \\ \hline
					0 & 1 & 1 \\ \hline
				\end{tabular}
				\caption{Dataset 3}
			\end{table}
			\item~[4 points] Given the dataset in Table 3, can you calculate its margin? If you cannot, please explain why. 

			% 2a 
			\textit{Answer:}

			Yes, dataset 3 has a margin as it is separable. 
			The margin is calculated using hyperplane $x_1 + x_2 = 0$ which gives the minimum margin of any hyperplane that separates the data.
			The margin is $\frac{1}{\sqrt{2}}$ or $0.707$.


			\begin{table}[h]
				\centering
				\begin{tabular}{cc|c}
					$x_1$ & $x_2$ &  {label}\\ 
					\hline\hline
						-1 & 0 & -1 \\ \hline
					0 & -1 & 1 \\ \hline
					1 & 0 & -1 \\ \hline
					0 & 1 & 1 \\ \hline
				\end{tabular}
				\caption{Dataset 4}
			\end{table}
			\item~[4 points] Given the dataset in Table 4, can you calculate its margin? If you cannot, please explain why. 

			% 2b 
			\textit{Answer:}
			No, dataset 4 is not separable therefore no margin can be found. 
			This is because when the points are plotted we see two data points with the same label on the $x_1$ axis and two data points with the same label (but opposite of the one on $x_1$) on the $x_2$ axis.
			
		\end{enumerate}
	
	\item ~[\textbf{Bonus}] [5 points] Let us review the Mistake Bound Theorem for Perceptron discussed in our lecture.  If we change the second assumption to be as follows: Suppose there exists a vector $\u\in \mathbb{R}^n$, and a positive $\gamma$, we have for each $(\x_i, y_i)$ in the training data, $y_i(\u^\top \x_i) \ge \gamma$. What is the upper bound for the number of mistakes made by the Perceptron algorithm?   Note that $\u$ is unnecessary to be a unit vector.
	
	\item~[10 points] We want to use Perceptron to learn a disjunction as follows,
	\[
	f(x_1, x_2, \ldots, x_n) = \neg x_1 \lor \neg \ldots \neg x_k \lor x_{k+1} \lor \ldots \lor x_{2k} \;\;\;\;(\mathrm{note\; that}\;\; 2k < n).
	\]
	The training set are all $2^n$ Boolean input vectors in the instance space. 
	Please derive an upper bound of the number of mistakes made by Perceptron in learning this disjunction.

	% 4 
	\textit{Answer:}

	Given that we can represent the disjunction as an equivalent linear classifier of 
	\[
		k + -x_1 + ... + -x_k + x_{k+1} + ... + x_{2k} \ge 1.
	\]
	We can then get a hyperplane that separates the data and no data points fall on it of 
	\[
		-x_1 + ... + -x_k + x_{k+1} + ... + x_{2k} - \frac{1}{2} + k = 0. 
	\]
	This gives a normalized $\u$ of 
	\[
		\u = \frac{1}{\sqrt{2k + (k - \frac{1}{2})^2}} [-1, ..., -1, 1, ..., 1, k-\frac{1}{2}]^\top.
	\]
	We have 
	\[
		R =\sqrt{1 + 2k}
	\]
	and 
	\[
		\gamma = \frac{k - \frac{1}{2}}{\sqrt{2k + (k - \frac{1}{2})^2}} = \frac{k - \frac{1}{2}}{k + \frac{1}{2}}.
	\]
	Thus the upper bound of the number of mistakes made by a Perceptron is
	\[
		\frac{R^2}{\gamma^2} = \frac{(1 + 2k)(k + \frac{1}{2})^2}{(k - \frac{1}{2})^2}.
	\]


	\item~[10 points] Prove that linear classifiers in a plane cannot shatter any $4$ distinct points. 

	% 5 
	\textit{Answer:}

	Given a set of 4 distinct points 
	\[
		S = \{[-1,0]^\top, [0,-1]^\top, [1,0]^\top, [0,1]^\top\}.
	\]

	Assume $S$ can be shattered by the set of linear classifiers 
	\[
		H = \{f | f = \w^\top \x \; \text{and} \; \w \in \mathbb{R}^2,  \x \in \mathbb{R}^2\}.
	\]

	Suppose we have a partition S into binary labels 1 and -1 respectively as follows 
	\[
		P = \{\{[0, -1]^\top, [0, 1]^\top\}, \{ [-1, 0]^\top, [1, 0]^\top\}\}
	\].

	There exists no $f \in H$ that gives labels of 1 to $\{[0, -1]^\top, [0, 1]^\top\}$ and labels of -1 to $\{ [-1, 0]^\top, [1, 0]^\top\}$ (There exists no $f \in H$ that gives $P$).

	Thus by contradiction, the set of linear classifiers cannot shatter any 4 distinct points.


	\item~[\textbf{Bonus}]~[10 points] Consider our infinite hypothesis space $\Hcal$ are all rectangles in a plain. Each rectangle corresponds to a classifier --- all the points inside the rectangle are classified as positive, and otherwise classified as negative. What is $\mathrm{VC}(\Hcal)$? 


\end{enumerate}

\section{Practice [64 points ]}
\begin{enumerate}
	\item~[2 Points] Update your machine learning library. Please check in your implementation of ensemble learning and least-mean-square (LMS) method in HW1 to your GitHub repository. Remember last time you created the folders ``Ensemble Learning" and ``Linear Regression''. You can commit your code into the corresponding folders now. Please also supplement README.md with concise descriptions about how to use your code to run your Adaboost, bagging, random forest, LMS with batch-gradient and stochastic gradient (how to call the command, set the parameters, etc). Please create a new folder ``Perceptron" in the same level as these folders.  

	% 1
	\textit{Answer:}

	GitHub commit for this homework: 

	\url{https://github.com/dalbyryan3/cs-6350-machine-learning/tree/ef576efb1bce90a53e773f055add21b715121744}

	Can follow README.md or can execute run.sh. 
	Results may not be exactly like shown in this assignment because of random shuffling that occurs during each training.


\item We will implement  Perceptron for a binary classification task --- bank-note authentication. Please download the data ``bank-note.zip'' from Canvas. The features and labels are listed in the file ``bank-note/data-desc.txt''. The training data are stored in the file ``bank-note/train.csv'', consisting of $872$ examples. The test data are stored in ``bank-note/test.csv'', and comprise of $500$ examples. In both the training and testing datasets, feature values and labels are separated by commas. 
\begin{enumerate}
	\item~[16 points] Implement the standard Perceptron. Set the maximum number of epochs $T$ to 10. Report your learned weight vector, and the average prediction error on the test dataset. 

	% 2a
	\textit{Answer:}

	Learned weight vector = [-0.47449984 -0.26700685 -0.19549265 -0.18584688]
	
	Training error = 0.05733944954128441, test error = 0.068

	Note for the standard Perceptron model the learned weight vector and resulting training and test error varied a lot over multiple runs of randomly shuffling the data.


	\item~[16 points] Implement the voted Perceptron. Set the maximum number of epochs $T$ to 10. Report the list of the distinct weight vectors and their counts --- the number of correctly predicted training examples. Using this set of weight vectors to predict each test example. Report the average test error. 

	% 2b
	\textit{Answer:}

	The voted Perceptron counts/votes and associated weight vectors can be seen in Table \ref{tab:2b}.

	{\tiny
	\begin{longtable}{|c|c|}
		\caption{Voted Perceptron Counts(Votes) and Weight Vectors}
		\label{tab:2b}\\
		\toprule
		 Counts &                            Weights \\
		\midrule
		\endfirsthead
		\caption[]{Voted Perceptron Counts(Votes) and Weight Vectors} \\
		\toprule
		 Counts &                            Weights \\
		\midrule
		\endhead
		\midrule
		\multicolumn{2}{r}{{Continued on next page}} \\
		\midrule
		\endfoot
		
		\bottomrule
		\endlastfoot
			  4 &    [-0.0156 -0.0221 0.0426 0.0028] \\
			  4 &    [-0.0145 0.0101 0.0083 -0.0257] \\
			  1 &   [-0.0123 -0.0702 -0.0106 0.0132] \\
			  3 &  [-0.0201 -0.0362 -0.0454 -0.0224] \\
			  2 &   [-0.0122 -0.1319 -0.0076 0.0527] \\
			  8 &   [-0.0565 -0.1084 -0.0246 0.0436] \\
			  2 &   [-0.0782 -0.0782 -0.0487 0.0193] \\
			  7 &  [-0.1179 -0.0413 -0.0547 -0.0208] \\
			  1 &   [-0.1000 0.0065 -0.1061 -0.0532] \\
			 11 &  [-0.1309 -0.0809 -0.0771 -0.0435] \\
			  1 &  [-0.1342 -0.0363 -0.1228 -0.0336] \\
			  1 &   [-0.1212 -0.1390 -0.0933 0.0250] \\
			  5 &   [-0.1245 -0.0944 -0.1390 0.0349] \\
			  1 &   [-0.1604 -0.1567 -0.0366 0.0234] \\
			 17 &   [-0.1930 -0.1121 -0.0746 0.0249] \\
			 20 &  [-0.1691 -0.0665 -0.1245 -0.0041] \\
			  4 &  [-0.1768 -0.1410 -0.0596 -0.0005] \\
			 21 &  [-0.1792 -0.1086 -0.0903 -0.0283] \\
			  1 &  [-0.1695 -0.0702 -0.1396 -0.0696] \\
			  4 &  [-0.1492 -0.0517 -0.1697 -0.0696] \\
			  3 &   [-0.1867 -0.1863 0.0062 -0.0973] \\
			 12 &  [-0.1834 -0.1531 -0.0388 -0.1375] \\
			  4 &  [-0.1732 -0.1441 -0.0624 -0.1332] \\
			 12 &  [-0.1531 -0.1261 -0.0919 -0.1311] \\
			  2 &  [-0.1496 -0.1249 -0.0948 -0.1296] \\
			 29 &  [-0.1440 -0.1280 -0.0930 -0.1252] \\
			 32 &  [-0.1389 -0.1232 -0.1128 -0.1194] \\
			  1 &  [-0.1315 -0.1610 -0.0967 -0.1036] \\
			  6 &  [-0.1325 -0.1428 -0.1188 -0.1093] \\
			  7 &  [-0.1552 -0.1438 -0.0907 -0.1044] \\
			 42 &  [-0.1514 -0.1520 -0.0828 -0.0970] \\
			  3 &  [-0.1714 -0.0834 -0.1642 -0.0946] \\
			  4 &  [-0.2182 -0.1400 -0.0545 -0.0979] \\
			  6 &  [-0.2391 -0.0719 -0.1391 -0.0919] \\
			  3 &  [-0.2375 -0.1055 -0.1254 -0.0783] \\
			  1 &  [-0.2110 -0.2068 -0.1120 -0.0236] \\
			 25 &  [-0.2396 -0.1617 -0.1179 -0.0663] \\
			  9 &  [-0.2375 -0.1686 -0.1091 -0.0633] \\
			  4 &  [-0.2253 -0.1476 -0.1410 -0.0620] \\
			 19 &  [-0.2265 -0.1416 -0.1449 -0.0626] \\
			 20 &  [-0.2106 -0.1195 -0.1761 -0.0638] \\
			  1 &  [-0.2553 -0.2498 -0.0052 -0.0941] \\
			  7 &  [-0.2404 -0.2155 -0.0456 -0.1084] \\
			 21 &  [-0.2378 -0.1656 -0.0971 -0.1723] \\
			  9 &  [-0.2377 -0.1593 -0.1035 -0.1647] \\
			 19 &  [-0.2398 -0.1661 -0.1025 -0.1528] \\
			 12 &  [-0.2395 -0.1767 -0.0885 -0.1450] \\
			 45 &  [-0.2398 -0.1784 -0.0896 -0.1328] \\
			 34 &  [-0.2346 -0.1758 -0.0980 -0.1232] \\
			 14 &  [-0.2195 -0.1562 -0.1286 -0.1244] \\
			  7 &  [-0.2261 -0.1842 -0.0915 -0.1145] \\
			  3 &  [-0.2116 -0.1482 -0.1321 -0.1304] \\
			  1 &  [-0.1943 -0.1086 -0.1795 -0.1554] \\
			  4 &  [-0.2118 -0.1668 -0.1208 -0.1433] \\
			 39 &  [-0.2170 -0.1342 -0.1517 -0.1335] \\
			  5 &  [-0.2157 -0.2374 -0.1145 -0.0723] \\
			 24 &  [-0.2373 -0.1686 -0.1961 -0.0715] \\
			 23 &  [-0.2490 -0.1829 -0.1668 -0.0649] \\
			 38 &  [-0.2465 -0.1681 -0.1810 -0.0707] \\
			  4 &  [-0.2297 -0.1261 -0.2264 -0.0946] \\
			  1 &  [-0.2360 -0.1891 -0.1786 -0.0836] \\
			  1 &  [-0.2412 -0.1564 -0.2095 -0.0737] \\
			  4 &  [-0.2833 -0.2812 -0.0598 -0.0876] \\
			  4 &  [-0.2885 -0.2485 -0.0907 -0.0778] \\
			 16 &  [-0.2910 -0.2292 -0.1154 -0.0858] \\
			  6 &  [-0.2890 -0.2100 -0.1482 -0.0920] \\
			  2 &  [-0.2800 -0.1623 -0.1966 -0.1479] \\
			  2 &  [-0.3084 -0.2286 -0.0918 -0.1521] \\
			 13 &  [-0.3062 -0.2338 -0.0958 -0.1401] \\
			  8 &  [-0.3002 -0.2238 -0.1179 -0.1391] \\
			  7 &  [-0.2942 -0.2044 -0.1508 -0.1424] \\
			 41 &  [-0.2836 -0.1926 -0.1772 -0.1413] \\
			 15 &  [-0.2853 -0.2044 -0.1634 -0.1339] \\
			 12 &  [-0.2751 -0.1934 -0.1864 -0.1280] \\
			  2 &  [-0.2593 -0.1847 -0.2095 -0.1198] \\
			  9 &  [-0.2458 -0.1741 -0.2330 -0.1158] \\
			  2 &  [-0.2944 -0.2331 -0.1232 -0.1240] \\
			 15 &  [-0.2724 -0.1876 -0.1729 -0.1512] \\
			 34 &  [-0.2712 -0.1854 -0.1777 -0.1415] \\
			  7 &  [-0.2872 -0.1893 -0.1615 -0.1224] \\
			 42 &  [-0.2816 -0.1924 -0.1596 -0.1180] \\
			 10 &  [-0.2657 -0.1703 -0.1908 -0.1191] \\
			 26 &  [-0.2774 -0.1845 -0.1616 -0.1125] \\
			  6 &  [-0.2774 -0.1782 -0.1680 -0.1049] \\
			  1 &  [-0.2600 -0.1387 -0.2154 -0.1300] \\
			 47 &  [-0.2960 -0.2041 -0.1102 -0.1349] \\
			 48 &  [-0.2757 -0.1855 -0.1403 -0.1348] \\
			 19 &  [-0.2518 -0.1400 -0.1902 -0.1638] \\
			  6 &  [-0.2827 -0.2063 -0.0848 -0.1727] \\
			 47 &  [-0.2705 -0.1854 -0.1167 -0.1714] \\
			 10 &  [-0.2654 -0.1806 -0.1365 -0.1657] \\
			 28 &  [-0.2552 -0.1695 -0.1595 -0.1597] \\
			  7 &  [-0.2417 -0.1589 -0.1830 -0.1557] \\
			  1 &  [-0.2618 -0.2261 -0.0928 -0.1547] \\
			 17 &  [-0.2593 -0.2114 -0.1070 -0.1606] \\
			 13 &  [-0.2448 -0.1753 -0.1475 -0.1765] \\
			 10 &  [-0.2500 -0.1427 -0.1784 -0.1667] \\
			  5 &  [-0.2488 -0.1405 -0.1832 -0.1570] \\
			  5 &  [-0.2815 -0.2679 -0.0276 -0.1584] \\
			 12 &  [-0.2856 -0.2625 -0.0329 -0.1518] \\
			  5 &  [-0.2724 -0.2434 -0.0660 -0.1512] \\
			  3 &  [-0.3012 -0.2028 -0.1022 -0.1479] \\
			 20 &  [-0.3082 -0.1479 -0.1856 -0.1192] \\
			 38 &  [-0.2977 -0.1360 -0.2120 -0.1181] \\
			  5 &  [-0.2942 -0.1348 -0.2149 -0.1166] \\
			  9 &  [-0.2939 -0.1453 -0.2009 -0.1089] \\
			  1 &  [-0.2942 -0.1470 -0.2020 -0.0967] \\
			  4 &  [-0.2920 -0.1540 -0.1932 -0.0937] \\
			  3 &  [-0.2842 -0.2496 -0.1554 -0.0187] \\
			 15 &  [-0.2865 -0.2172 -0.1860 -0.0465] \\
			 17 &  [-0.2898 -0.1727 -0.2318 -0.0366] \\
			  4 &  [-0.3152 -0.2423 -0.1437 -0.0213] \\
			  4 &  [-0.3092 -0.2229 -0.1766 -0.0246] \\
			 36 &  [-0.3204 -0.1896 -0.1900 -0.0441] \\
			 14 &  [-0.3081 -0.1493 -0.2365 -0.0833] \\
			 49 &  [-0.3252 -0.1970 -0.1744 -0.0793] \\
			  1 &  [-0.3269 -0.2089 -0.1605 -0.0719] \\
			  1 &  [-0.3302 -0.1643 -0.2063 -0.0621] \\
			 20 &  [-0.3379 -0.1831 -0.1822 -0.0507] \\
			 11 &  [-0.3231 -0.1488 -0.2226 -0.0650] \\
			  1 &  [-0.3566 -0.2212 -0.1081 -0.0707] \\
			  6 &  [-0.3465 -0.2122 -0.1316 -0.0664] \\
			  7 &  [-0.3307 -0.2035 -0.1548 -0.0582] \\
			  4 &  [-0.3408 -0.1735 -0.1664 -0.0744] \\
			  1 &  [-0.3240 -0.1314 -0.2118 -0.0983] \\
			  5 &  [-0.3642 -0.2145 -0.0863 -0.1134] \\
			 34 &  [-0.3684 -0.1854 -0.1041 -0.1355] \\
			  1 &  [-0.3668 -0.1208 -0.1877 -0.1203] \\
			  5 &  [-0.3517 -0.1012 -0.2183 -0.1215] \\
			 15 &  [-0.3674 -0.1233 -0.1757 -0.1187] \\
			  5 &  [-0.3472 -0.1053 -0.2053 -0.1166] \\
			  6 &  [-0.3910 -0.1605 -0.0959 -0.1207] \\
			  4 &  [-0.3845 -0.1144 -0.1794 -0.0936] \\
			  4 &  [-0.3823 -0.1196 -0.1834 -0.0816] \\
			  8 &  [-0.3785 -0.1279 -0.1756 -0.0741] \\
			 45 &  [-0.3840 -0.2071 -0.1084 -0.0667] \\
			  8 &  [-0.3892 -0.1745 -0.1393 -0.0568] \\
			  9 &  [-0.3886 -0.1495 -0.1687 -0.0630] \\
			 14 &  [-0.3834 -0.1469 -0.1771 -0.0534] \\
			  4 &  [-0.3615 -0.1014 -0.2269 -0.0807] \\
			 20 &  [-0.3418 -0.2195 -0.2228 -0.0020] \\
			  1 &  [-0.3388 -0.1706 -0.2743 -0.0643] \\
			  4 &  [-0.3844 -0.2965 -0.1199 -0.0793] \\
			  1 &  [-0.4021 -0.2622 -0.1320 -0.1031] \\
			 27 &  [-0.4073 -0.2295 -0.1629 -0.0932] \\
			 15 &  [-0.3894 -0.1817 -0.2143 -0.1256] \\
			 15 &  [-0.3793 -0.1727 -0.2378 -0.1213] \\
			 15 &  [-0.3718 -0.2104 -0.2217 -0.1055] \\
			  9 &  [-0.3666 -0.2078 -0.2301 -0.0959] \\
			 33 &  [-0.3561 -0.1960 -0.2565 -0.0948] \\
			  7 &  [-0.3762 -0.2632 -0.1663 -0.0938] \\
			  5 &  [-0.3693 -0.2147 -0.2185 -0.1549] \\
			 29 &  [-0.3690 -0.2252 -0.2044 -0.1471] \\
			 17 &  [-0.3489 -0.2072 -0.2340 -0.1450] \\
			 34 &  [-0.3433 -0.2103 -0.2322 -0.1406] \\
			  6 &  [-0.3193 -0.1647 -0.2821 -0.1696] \\
			  1 &  [-0.3317 -0.1817 -0.2569 -0.1644] \\
			 15 &  [-0.3052 -0.2830 -0.2436 -0.1097] \\
			  1 &  [-0.3669 -0.1959 -0.2457 -0.1460] \\
			  7 &  [-0.3520 -0.1616 -0.2861 -0.1603] \\
			  3 &  [-0.3680 -0.1655 -0.2699 -0.1412] \\
			  4 &  [-0.3746 -0.1936 -0.2327 -0.1312] \\
			  5 &  [-0.3601 -0.1575 -0.2733 -0.1472] \\
			  5 &  [-0.3336 -0.2589 -0.2600 -0.0925] \\
			  5 &  [-0.3348 -0.2528 -0.2638 -0.0931] \\
			 13 &  [-0.3504 -0.2749 -0.2213 -0.0903] \\
			  5 &  [-0.3398 -0.2380 -0.2628 -0.1096] \\
			  3 &  [-0.3262 -0.2274 -0.2863 -0.1056] \\
			 11 &  [-0.3622 -0.2897 -0.1839 -0.1172] \\
			 12 &  [-0.3501 -0.2489 -0.2315 -0.1433] \\
			 27 &  [-0.3617 -0.2632 -0.2023 -0.1367] \\
			  1 &  [-0.3557 -0.2532 -0.2244 -0.1357] \\
			 24 &  [-0.3354 -0.2346 -0.2545 -0.1357] \\
			 14 &  [-0.3222 -0.2156 -0.2876 -0.1350] \\
			  9 &  [-0.3063 -0.1935 -0.3188 -0.1362] \\
			 28 &  [-0.3531 -0.2501 -0.2091 -0.1396] \\
			  5 &  [-0.3467 -0.2041 -0.2926 -0.1125] \\
			 14 &  [-0.3869 -0.2871 -0.1671 -0.1276] \\
			 33 &  [-0.3939 -0.2321 -0.2504 -0.0988] \\
			  2 &  [-0.3915 -0.2174 -0.2646 -0.1047] \\
			 25 &  [-0.3992 -0.2362 -0.2406 -0.0934] \\
			 41 &  [-0.3890 -0.2251 -0.2636 -0.0874] \\
			  3 &  [-0.4226 -0.2976 -0.1492 -0.0932] \\
			 15 &  [-0.4075 -0.2780 -0.1798 -0.0944] \\
			 25 &  [-0.4108 -0.2334 -0.2255 -0.0845] \\
			 41 &  [-0.4126 -0.2452 -0.2117 -0.0772] \\
			  6 &  [-0.3906 -0.1997 -0.2614 -0.1044] \\
			  2 &  [-0.3738 -0.1577 -0.3068 -0.1283] \\
			  2 &  [-0.3913 -0.2159 -0.2481 -0.1162] \\
			 45 &  [-0.3875 -0.2241 -0.2403 -0.1088] \\
			 34 &  [-0.3753 -0.2031 -0.2722 -0.1075] \\
			  4 &  [-0.4062 -0.2695 -0.1668 -0.1164] \\
			  3 &  [-0.4062 -0.2632 -0.1732 -0.1088] \\
			 80 &  [-0.4163 -0.2332 -0.1849 -0.1250] \\
			 36 &  [-0.4165 -0.2350 -0.1860 -0.1128] \\
			  3 &  [-0.4131 -0.2337 -0.1889 -0.1113] \\
			 13 &  [-0.4057 -0.2164 -0.2209 -0.1128] \\
			  1 &  [-0.3899 -0.2077 -0.2440 -0.1046] \\
			 22 &  [-0.3877 -0.2147 -0.2352 -0.1016] \\
			 53 &  [-0.3865 -0.2124 -0.2400 -0.0919] \\
			  7 &  [-0.3844 -0.2194 -0.2312 -0.0889] \\
			  6 &  [-0.3624 -0.1739 -0.2810 -0.1162] \\
			 30 &  [-0.3360 -0.2753 -0.2676 -0.0615] \\
			  7 &  [-0.3253 -0.2383 -0.3092 -0.0808] \\
			  4 &  [-0.3094 -0.2162 -0.3404 -0.0820] \\
			  1 &  [-0.3354 -0.3097 -0.2425 -0.0848] \\
			  5 &  [-0.3351 -0.3202 -0.2285 -0.0771] \\
			 13 &  [-0.3360 -0.3163 -0.2317 -0.0785] \\
			  5 &  [-0.3363 -0.3180 -0.2329 -0.0663] \\
			  3 &  [-0.3415 -0.2854 -0.2638 -0.0564] \\
			  2 &  [-0.4048 -0.1925 -0.2636 -0.1243] \\
			 14 &  [-0.4188 -0.2892 -0.1690 -0.1278] \\
			 11 &  [-0.4200 -0.2832 -0.1728 -0.1283] \\
			 42 &  [-0.4110 -0.2354 -0.2213 -0.1842] \\
			  8 &  [-0.4176 -0.2677 -0.1832 -0.1724] \\
			 17 &  [-0.4164 -0.2655 -0.1879 -0.1627] \\
			  6 &  [-0.3996 -0.2234 -0.2333 -0.1866] \\
			  5 &  [-0.3757 -0.1778 -0.2832 -0.2156] \\
			 68 &  [-0.3909 -0.2426 -0.2257 -0.2069] \\
			 34 &  [-0.3760 -0.2083 -0.2660 -0.2212] \\
			 44 &  [-0.3681 -0.3040 -0.2281 -0.1461] \\
			 21 &  [-0.3617 -0.2579 -0.3116 -0.1190] \\
			  4 &  [-0.3977 -0.3233 -0.2063 -0.1239] \\
			 11 &  [-0.3977 -0.3219 -0.2083 -0.1238] \\
			 12 &  [-0.3887 -0.2882 -0.2533 -0.1608] \\
			 15 &  [-0.3852 -0.2870 -0.2562 -0.1593] \\
			  5 &  [-0.3730 -0.2660 -0.2881 -0.1581] \\
			 23 &  [-0.3629 -0.2570 -0.3116 -0.1538] \\
			  3 &  [-0.3494 -0.2464 -0.3351 -0.1498] \\
			 63 &  [-0.3808 -0.3768 -0.1783 -0.1564] \\
			  6 &  [-0.3909 -0.3468 -0.1899 -0.1726] \\
			 15 &  [-0.4115 -0.3084 -0.1979 -0.1847] \\
			 15 &  [-0.4318 -0.2699 -0.2042 -0.1965] \\
			 33 &  [-0.4258 -0.2599 -0.2264 -0.1955] \\
			  3 &  [-0.4290 -0.2154 -0.2721 -0.1856] \\
			  7 &  [-0.4089 -0.1974 -0.3017 -0.1835] \\
			 11 &  [-0.4269 -0.2655 -0.2347 -0.1718] \\
			 11 &  [-0.4269 -0.2592 -0.2411 -0.1642] \\
			 24 &  [-0.4167 -0.2482 -0.2641 -0.1583] \\
			  3 &  [-0.4022 -0.2121 -0.3046 -0.1743] \\
			  1 &  [-0.4330 -0.2785 -0.1992 -0.1832] \\
			 28 &  [-0.4293 -0.2867 -0.1914 -0.1757] \\
			  3 &  [-0.4142 -0.2671 -0.2219 -0.1770] \\
			  4 &  [-0.4194 -0.2345 -0.2528 -0.1671] \\
			 11 &  [-0.4211 -0.2463 -0.2390 -0.1598] \\
			 20 &  [-0.4106 -0.2345 -0.2654 -0.1587] \\
			  1 &  [-0.3948 -0.2258 -0.2886 -0.1504] \\
			 30 &  [-0.4114 -0.2973 -0.2096 -0.1408] \\
			 13 &  [-0.4044 -0.2956 -0.2275 -0.1371] \\
			  3 &  [-0.4020 -0.2809 -0.2417 -0.1430] \\
			  7 &  [-0.3846 -0.2413 -0.2891 -0.1680] \\
			  3 &  [-0.3824 -0.2465 -0.2931 -0.1560] \\
			 11 &  [-0.4147 -0.3187 -0.1767 -0.1655] \\
			  1 &  [-0.4091 -0.3087 -0.1994 -0.1655] \\
			  8 &  [-0.3970 -0.2679 -0.2471 -0.1917] \\
			  3 &  [-0.3918 -0.2653 -0.2555 -0.1820] \\
			 21 &  [-0.3715 -0.2468 -0.2856 -0.1820] \\
			 15 &  [-0.3875 -0.2507 -0.2694 -0.1629] \\
			 12 &  [-0.3797 -0.3464 -0.2315 -0.0879] \\
			  8 &  [-0.3770 -0.2965 -0.2830 -0.1518] \\
			  7 &  [-0.3638 -0.2775 -0.3162 -0.1511] \\
			  5 &  [-0.4013 -0.4121 -0.1402 -0.1789] \\
			  3 &  [-0.4042 -0.3803 -0.1760 -0.2108] \\
			  9 &  [-0.3919 -0.3400 -0.2224 -0.2499] \\
			  3 &  [-0.3863 -0.3431 -0.2206 -0.2455] \\
			 36 &  [-0.3915 -0.3104 -0.2515 -0.2356] \\
			  1 &  [-0.3914 -0.3041 -0.2579 -0.2280] \\
			 16 &  [-0.3936 -0.3110 -0.2569 -0.2161] \\
			  6 &  [-0.3920 -0.2463 -0.3405 -0.2008] \\
			  1 &  [-0.4013 -0.2974 -0.2951 -0.1870] \\
			 24 &  [-0.3835 -0.2496 -0.3465 -0.2193] \\
			 46 &  [-0.4091 -0.3184 -0.2711 -0.2123] \\
			 12 &  [-0.4094 -0.3201 -0.2722 -0.2001] \\
			 28 &  [-0.4146 -0.2875 -0.3031 -0.1902] \\
			  8 &  [-0.4223 -0.3063 -0.2791 -0.1789] \\
			 11 &  [-0.4078 -0.2702 -0.3196 -0.1949] \\
			 15 &  [-0.4044 -0.2689 -0.3225 -0.1934] \\
			 35 &  [-0.4238 -0.3558 -0.2310 -0.1840] \\
			 27 &  [-0.4443 -0.3174 -0.2389 -0.1961] \\
			  6 &  [-0.4294 -0.2831 -0.2792 -0.2104] \\
			 45 &  [-0.4121 -0.2436 -0.3266 -0.2354] \\
			  7 &  [-0.4322 -0.3108 -0.2365 -0.2344] \\
			  4 &  [-0.4333 -0.3047 -0.2403 -0.2350] \\
			 11 &  [-0.4131 -0.2867 -0.2699 -0.2329] \\
			 12 &  [-0.3996 -0.2761 -0.2934 -0.2289] \\
			 11 &  [-0.4157 -0.2800 -0.2772 -0.2098] \\
			 12 &  [-0.4078 -0.3757 -0.2393 -0.1348] \\
			 10 &  [-0.3957 -0.3350 -0.2869 -0.1609] \\
			 58 &  [-0.4009 -0.3023 -0.3178 -0.1510] \\
			  4 &  [-0.3770 -0.2568 -0.3677 -0.1800] \\
			  4 &  [-0.3836 -0.2848 -0.3306 -0.1701] \\
			  6 &  [-0.4273 -0.3399 -0.2212 -0.1741] \\
			 19 &  [-0.4261 -0.3377 -0.2259 -0.1644] \\
			  2 &  [-0.4103 -0.3290 -0.2491 -0.1562] \\
			  5 &  [-0.4082 -0.3360 -0.2403 -0.1532] \\
			 36 &  [-0.4082 -0.3346 -0.2423 -0.1532] \\
			 42 &  [-0.4164 -0.3050 -0.2551 -0.1678] \\
			 19 &  [-0.4033 -0.2860 -0.2882 -0.1671] \\
			 29 &  [-0.3911 -0.2650 -0.3202 -0.1659] \\
			 49 &  [-0.4195 -0.3313 -0.2154 -0.1701] \\
			 11 &  [-0.4397 -0.2928 -0.2217 -0.1818] \\
			 20 &  [-0.4238 -0.2707 -0.2529 -0.1830] \\
			 13 &  [-0.4182 -0.2738 -0.2510 -0.1785] \\
			 36 &  [-0.4179 -0.2843 -0.2370 -0.1708] \\
			  4 &  [-0.4212 -0.2397 -0.2827 -0.1609] \\
			  4 &  [-0.4175 -0.2480 -0.2749 -0.1535] \\
			 12 &  [-0.4291 -0.2622 -0.2456 -0.1468] \\
			  1 &  [-0.4343 -0.2296 -0.2765 -0.1370] \\
			  5 &  [-0.4124 -0.1841 -0.3263 -0.1642] \\
			  2 &  [-0.4636 -0.2372 -0.2225 -0.1749] \\
			 13 &  [-0.4433 -0.2187 -0.2526 -0.1748] \\
			  2 &  [-0.4331 -0.2077 -0.2756 -0.1689] \\
			 15 &  [-0.4309 -0.2129 -0.2796 -0.1569] \\
			 34 &  [-0.4465 -0.2350 -0.2370 -0.1541] \\
			 17 &  [-0.4297 -0.1929 -0.2824 -0.1780] \\
			  3 &  [-0.4481 -0.2838 -0.1900 -0.1790] \\
			 13 &  [-0.4533 -0.2512 -0.2209 -0.1692] \\
			  1 &  [-0.4382 -0.2316 -0.2515 -0.1704] \\
			 17 &  [-0.4399 -0.2434 -0.2377 -0.1631] \\
			 24 &  [-0.4347 -0.2408 -0.2461 -0.1535] \\
			 25 &  [-0.4269 -0.3365 -0.2082 -0.0784] \\
			 28 &  [-0.4280 -0.3304 -0.2121 -0.0790] \\
			 11 &  [-0.4224 -0.3335 -0.2102 -0.0746] \\
			 25 &  [-0.4483 -0.2949 -0.2136 -0.0874] \\
			  5 &  [-0.4595 -0.2615 -0.2270 -0.1069] \\
			  7 &  [-0.4416 -0.2137 -0.2784 -0.1393] \\
			  7 &  [-0.4413 -0.2242 -0.2644 -0.1316] \\
			 21 &  [-0.4491 -0.2430 -0.2403 -0.1202] \\
			 10 &  [-0.4271 -0.1975 -0.2901 -0.1475] \\
			  4 &  [-0.4501 -0.2701 -0.2105 -0.1383] \\
			 28 &  [-0.4327 -0.2305 -0.2579 -0.1633] \\
			  8 &  [-0.4253 -0.2682 -0.2418 -0.1476] \\
			  3 &  [-0.4255 -0.2700 -0.2429 -0.1354] \\
			 36 &  [-0.4195 -0.2600 -0.2650 -0.1344] \\
			  3 &  [-0.4036 -0.2379 -0.2962 -0.1356] \\
			 54 &  [-0.4162 -0.2526 -0.2675 -0.1311] \\
			  2 &  [-0.4013 -0.2183 -0.3078 -0.1453] \\
			 24 &  [-0.4079 -0.2463 -0.2707 -0.1354] \\
			 11 &  [-0.3928 -0.2267 -0.3013 -0.1366] \\
			  3 &  [-0.4376 -0.3570 -0.1305 -0.1669] \\
			  6 &  [-0.4208 -0.3150 -0.1759 -0.1909] \\
			 19 &  [-0.4241 -0.2704 -0.2216 -0.1810] \\
			 23 &  [-0.4184 -0.2604 -0.2443 -0.1810] \\
			 30 &  [-0.3945 -0.2148 -0.2942 -0.2100] \\
			 61 &  [-0.4084 -0.2636 -0.2294 -0.2066] \\
			 25 &  [-0.4154 -0.2974 -0.1882 -0.1916] \\
			 17 &  [-0.4103 -0.2926 -0.2080 -0.1858] \\
			  4 &  [-0.4155 -0.2600 -0.2389 -0.1759] \\
			 12 &  [-0.4121 -0.2587 -0.2418 -0.1745] \\
			 17 &  [-0.3989 -0.2397 -0.2749 -0.1738] \\
			  6 &  [-0.4006 -0.2515 -0.2611 -0.1665] \\
			  3 &  [-0.3861 -0.2155 -0.3016 -0.1825] \\
			 16 &  [-0.3839 -0.2207 -0.3057 -0.1704] \\
			  7 &  [-0.3979 -0.3174 -0.2110 -0.1739] \\
			  3 &  [-0.3966 -0.3151 -0.2157 -0.1642] \\
			 36 &  [-0.3960 -0.2902 -0.2451 -0.1704] \\
			 35 &  [-0.3854 -0.2784 -0.2716 -0.1693] \\
			 11 &  [-0.3652 -0.2604 -0.3011 -0.1672] \\
			  4 &  [-0.3449 -0.2419 -0.3313 -0.1672] \\
			  2 &  [-0.3709 -0.3354 -0.2333 -0.1700] \\
			 17 &  [-0.3551 -0.3267 -0.2565 -0.1618] \\
			  5 &  [-0.3526 -0.3119 -0.2706 -0.1676] \\
			 59 &  [-0.3405 -0.2909 -0.3026 -0.1663] \\
			  2 &  [-0.3353 -0.2884 -0.3110 -0.1567] \\
			  5 &  [-0.3218 -0.2778 -0.3344 -0.1527] \\
			  4 &  [-0.3553 -0.3502 -0.2200 -0.1584] \\
			 19 &  [-0.3516 -0.3584 -0.2122 -0.1510] \\
			  9 &  [-0.3525 -0.3545 -0.2155 -0.1524] \\
			 32 &  [-0.3525 -0.3531 -0.2174 -0.1523] \\
			  2 &  [-0.3504 -0.3600 -0.2086 -0.1493] \\
			 55 &  [-0.3413 -0.3263 -0.2536 -0.1863] \\
			  3 &  [-0.3413 -0.3200 -0.2600 -0.1787] \\
			 14 &  [-0.3465 -0.2874 -0.2909 -0.1689] \\
			  7 &  [-0.3462 -0.2979 -0.2769 -0.1611] \\
			  6 &  [-0.3514 -0.2653 -0.3078 -0.1513] \\
			  2 &  [-0.3874 -0.4019 -0.1318 -0.1762] \\
			  1 &  [-0.3839 -0.4006 -0.1346 -0.1747] \\
			 15 &  [-0.3819 -0.3815 -0.1675 -0.1809] \\
			 32 &  [-0.4022 -0.3430 -0.1738 -0.1927] \\
			  2 &  [-0.4227 -0.3046 -0.1818 -0.2048] \\
			  1 &  [-0.4082 -0.2685 -0.2223 -0.2208] \\
			  2 &  [-0.4070 -0.2663 -0.2271 -0.2111] \\
			 16 &  [-0.3935 -0.2557 -0.2505 -0.2071] \\
			  2 &  [-0.3732 -0.2372 -0.2806 -0.2070] \\
			  9 &  [-0.3868 -0.2353 -0.2716 -0.1912] \\
			 39 &  [-0.3938 -0.2691 -0.2303 -0.1762] \\
			  5 &  [-0.3789 -0.2348 -0.2707 -0.1904] \\
			 23 &  [-0.3715 -0.2726 -0.2545 -0.1747] \\
			 19 &  [-0.3564 -0.2530 -0.2851 -0.1759] \\
			 21 &  [-0.3526 -0.2612 -0.2773 -0.1685] \\
			  2 &  [-0.3448 -0.3569 -0.2394 -0.0934] \\
			 10 &  [-0.3490 -0.3278 -0.2572 -0.1155] \\
			  2 &  [-0.3389 -0.3187 -0.2808 -0.1112] \\
			  2 &  [-0.3421 -0.2742 -0.3265 -0.1013] \\
			 13 &  [-0.3578 -0.2963 -0.2839 -0.0985] \\
			 36 &  [-0.3577 -0.2900 -0.2903 -0.0910] \\
			  9 &  [-0.3643 -0.3223 -0.2522 -0.0791] \\
			 18 &  [-0.3547 -0.2839 -0.3015 -0.1204] \\
			  7 &  [-0.3368 -0.2361 -0.3529 -0.1528] \\
			  7 &  [-0.3210 -0.2274 -0.3760 -0.1446] \\
			  3 &  [-0.3409 -0.3224 -0.2792 -0.1459] \\
			 28 &  [-0.3510 -0.2924 -0.2909 -0.1620] \\
			  2 &  [-0.3271 -0.2468 -0.3408 -0.1910] \\
			 19 &  [-0.3456 -0.2437 -0.3330 -0.1768] \\
			 10 &  [-0.3602 -0.2594 -0.3087 -0.1719] \\
			  1 &  [-0.3756 -0.3104 -0.2419 -0.1701] \\
			 22 &  [-0.3554 -0.2924 -0.2715 -0.1680] \\
			 10 &  [-0.3452 -0.2814 -0.2945 -0.1621] \\
			  4 &  [-0.3591 -0.3301 -0.2297 -0.1586] \\
			  7 &  [-0.3485 -0.3183 -0.2561 -0.1575] \\
			 27 &  [-0.3364 -0.2775 -0.3038 -0.1837] \\
			  4 &  [-0.3205 -0.2554 -0.3349 -0.1848] \\
			 10 &  [-0.3427 -0.2680 -0.3050 -0.1812] \\
			 26 &  [-0.3405 -0.2732 -0.3090 -0.1692] \\
			 21 &  [-0.3765 -0.3329 -0.2081 -0.1775] \\
			 15 &  [-0.3817 -0.3003 -0.2390 -0.1676] \\
			  3 &  [-0.3796 -0.3072 -0.2302 -0.1647] \\
			 46 &  [-0.3878 -0.2776 -0.2431 -0.1793] \\
			 24 &  [-0.3772 -0.2407 -0.2847 -0.1987] \\
			  4 &  [-0.3973 -0.3079 -0.1945 -0.1977] \\
			  1 &  [-0.3805 -0.2658 -0.2399 -0.2216] \\
			  2 &  [-0.3585 -0.2203 -0.2897 -0.2489] \\
			  5 &  [-0.3752 -0.2918 -0.2107 -0.2392] \\
			  1 &  [-0.3769 -0.3036 -0.1969 -0.2319] \\
			 27 &  [-0.3647 -0.2827 -0.2289 -0.2306] \\
			 20 &  [-0.3713 -0.3107 -0.1917 -0.2206] \\
			 19 &  [-0.3697 -0.2461 -0.2753 -0.2054] \\
			 57 &  [-0.3923 -0.2470 -0.2472 -0.2005] \\
			 53 &  [-0.3872 -0.2445 -0.2556 -0.1909] \\
			 64 &  [-0.3816 -0.2476 -0.2538 -0.1864] \\
			 28 &  [-0.3818 -0.2493 -0.2549 -0.1742] \\
			  5 &  [-0.3616 -0.2313 -0.2845 -0.1721] \\
			  9 &  [-0.3694 -0.2501 -0.2605 -0.1608] \\
			 11 &  [-0.3764 -0.2839 -0.2193 -0.1458] \\
			 10 &  [-0.3699 -0.2378 -0.3027 -0.1187] \\
			  6 &  [-0.3643 -0.2409 -0.3009 -0.1142] \\
			 38 &  [-0.3631 -0.2387 -0.3056 -0.1045] \\
			  4 &  [-0.4104 -0.3005 -0.1917 -0.1153] \\
			 18 &  [-0.4067 -0.3087 -0.1839 -0.1078] \\
			 54 &  [-0.4168 -0.2787 -0.1956 -0.1240] \\
			  2 &  [-0.4134 -0.2775 -0.1984 -0.1225] \\
			 31 &  [-0.4083 -0.2727 -0.2182 -0.1168] \\
			  2 &  [-0.3957 -0.2240 -0.2711 -0.1755] \\
			  2 &  [-0.3692 -0.3253 -0.2578 -0.1208] \\
			 36 &  [-0.3560 -0.3063 -0.2909 -0.1201] \\
			  2 &  [-0.3717 -0.3284 -0.2483 -0.1173] \\
			 31 &  [-0.3566 -0.3088 -0.2789 -0.1186] \\
			 25 &  [-0.3393 -0.2693 -0.3263 -0.1436] \\
			  6 &  [-0.3753 -0.3347 -0.2211 -0.1485] \\
			  6 &  [-0.3794 -0.3293 -0.2263 -0.1419] \\
			 36 &  [-0.3996 -0.2908 -0.2327 -0.1536] \\
			 24 &  [-0.4008 -0.2847 -0.2365 -0.1542] \\
			 21 &  [-0.4060 -0.2521 -0.2674 -0.1444] \\
			  3 &  [-0.3821 -0.2066 -0.3173 -0.1734] \\
			 13 &  [-0.3820 -0.2003 -0.3237 -0.1658] \\
			 40 &  [-0.4258 -0.2554 -0.2143 -0.1699] \\
			 14 &  [-0.4242 -0.1908 -0.2979 -0.1547] \\
			  1 &  [-0.4601 -0.2531 -0.1955 -0.1662] \\
			 42 &  [-0.4443 -0.2444 -0.2187 -0.1580] \\
			  1 &  [-0.4341 -0.2334 -0.2417 -0.1520] \\
			  3 &  [-0.4192 -0.1991 -0.2820 -0.1663] \\
			 28 &  [-0.4258 -0.2313 -0.2439 -0.1544] \\
			 40 &  [-0.4055 -0.2128 -0.2740 -0.1544] \\
			 15 &  [-0.4415 -0.3494 -0.0980 -0.1793] \\
			 12 &  [-0.4325 -0.3157 -0.1430 -0.2163] \\
			  7 &  [-0.4268 -0.3057 -0.1657 -0.2164] \\
			  4 &  [-0.4162 -0.2687 -0.2073 -0.2357] \\
			 16 &  [-0.4083 -0.3644 -0.1694 -0.1607] \\
			 11 &  [-0.4031 -0.3618 -0.1778 -0.1511] \\
			  3 &  [-0.3872 -0.3397 -0.2090 -0.1523] \\
			  5 &  [-0.3984 -0.3063 -0.2225 -0.1718] \\
			 16 &  [-0.3960 -0.2916 -0.2367 -0.1777] \\
			  2 &  [-0.3859 -0.2826 -0.2602 -0.1734] \\
			  3 &  [-0.3876 -0.2944 -0.2464 -0.1661] \\
			  6 &  [-0.3873 -0.3049 -0.2323 -0.1584] \\
			 62 &  [-0.3750 -0.2646 -0.2788 -0.1975] \\
			  8 &  [-0.3531 -0.2191 -0.3285 -0.2247] \\
			 54 &  [-0.4016 -0.2781 -0.2187 -0.2329] \\
			  5 &  [-0.4019 -0.2799 -0.2198 -0.2208] \\
			  2 &  [-0.3996 -0.2851 -0.2239 -0.2087] \\
			 23 &  [-0.4048 -0.2524 -0.2548 -0.1989] \\
			  3 &  [-0.4081 -0.2079 -0.3005 -0.1890] \\
			  2 &  [-0.4408 -0.3353 -0.1449 -0.1904] \\
			  3 &  [-0.4286 -0.3143 -0.1769 -0.1891] \\
			 49 &  [-0.4151 -0.3037 -0.2003 -0.1851] \\
			 12 &  [-0.4081 -0.3020 -0.2182 -0.1815] \\
			 53 &  [-0.4058 -0.3072 -0.2222 -0.1695] \\
			 11 &  [-0.3935 -0.2669 -0.2686 -0.2086] \\
			 12 &  [-0.3901 -0.2657 -0.2715 -0.2072] \\
			 22 &  [-0.3699 -0.2477 -0.3011 -0.2051] \\
			 28 &  [-0.3717 -0.2595 -0.2873 -0.1977] \\
			 68 &  [-0.3794 -0.2783 -0.2633 -0.1864] \\
			 11 &  [-0.3555 -0.2327 -0.3131 -0.2154] \\
			  5 &  [-0.3290 -0.3341 -0.2998 -0.1607] \\
			  6 &  [-0.3111 -0.2863 -0.3512 -0.1930] \\
			 11 &  [-0.3296 -0.2832 -0.3435 -0.1789] \\
			 10 &  [-0.3508 -0.2837 -0.3240 -0.1653] \\
			  7 &  [-0.3456 -0.2812 -0.3324 -0.1557] \\
			 28 &  [-0.3378 -0.3768 -0.2945 -0.0807] \\
			  1 &  [-0.3246 -0.3578 -0.3276 -0.0800] \\
			  7 &  [-0.3402 -0.3799 -0.2850 -0.0772] \\
			  9 &  [-0.3313 -0.3322 -0.3335 -0.1331] \\
			  8 &  [-0.3429 -0.3464 -0.3042 -0.1265] \\
			 14 &  [-0.3946 -0.2660 -0.3038 -0.1715] \\
			  3 &  [-0.3811 -0.2554 -0.3272 -0.1675] \\
			 15 &  [-0.4037 -0.3028 -0.2637 -0.1664] \\
			  1 &  [-0.4015 -0.3098 -0.2550 -0.1634] \\
			 20 &  [-0.3960 -0.3129 -0.2531 -0.1590] \\
			 17 &  [-0.3943 -0.2483 -0.3367 -0.1438] \\
			  2 &  [-0.3940 -0.2588 -0.3227 -0.1360] \\
			 16 &  [-0.3781 -0.2366 -0.3539 -0.1372] \\
			  6 &  [-0.3844 -0.2997 -0.3060 -0.1261] \\
			  4 &  [-0.4070 -0.3006 -0.2779 -0.1213] \\
			 39 &  [-0.3920 -0.2810 -0.3085 -0.1225] \\
			  9 &  [-0.3922 -0.2828 -0.3096 -0.1103] \\
			  7 &  [-0.4206 -0.3491 -0.2048 -0.1145] \\
			  6 &  [-0.4182 -0.3344 -0.2190 -0.1204] \\
			  1 &  [-0.4181 -0.3281 -0.2254 -0.1128] \\
			  5 &  [-0.4008 -0.2885 -0.2728 -0.1378] \\
			 39 &  [-0.3850 -0.2798 -0.2959 -0.1295] \\
			 14 &  [-0.3647 -0.2613 -0.3260 -0.1295] \\
			 47 &  [-0.3900 -0.3309 -0.2380 -0.1142] \\
			 31 &  [-0.3794 -0.2939 -0.2796 -0.1336] \\
			 28 &  [-0.3645 -0.2597 -0.3199 -0.1479] \\
			  5 &  [-0.3972 -0.3871 -0.1643 -0.1493] \\
			 13 &  [-0.3804 -0.3450 -0.2097 -0.1732] \\
			 45 &  [-0.3792 -0.3428 -0.2144 -0.1635] \\
			  3 &  [-0.3834 -0.3137 -0.2323 -0.1856] \\
			  2 &  [-0.3970 -0.3118 -0.2232 -0.1698] \\
			  3 &  [-0.3910 -0.2925 -0.2561 -0.1730] \\
			 18 &  [-0.3873 -0.3007 -0.2483 -0.1656] \\
			 15 &  [-0.3884 -0.2947 -0.2521 -0.1662] \\
			  3 &  [-0.3783 -0.2837 -0.2751 -0.1602] \\
			  3 &  [-0.3563 -0.2382 -0.3249 -0.1875] \\
			 64 &  [-0.3763 -0.3332 -0.2281 -0.1888] \\
			  1 &  [-0.3641 -0.3122 -0.2600 -0.1875] \\
			  4 &  [-0.3576 -0.2661 -0.3435 -0.1604] \\
			 13 &  [-0.3912 -0.3385 -0.2291 -0.1661] \\
			 21 &  [-0.3791 -0.2978 -0.2767 -0.1922] \\
			 25 &  [-0.3690 -0.2888 -0.3002 -0.1879] \\
			  5 &  [-0.3916 -0.2897 -0.2721 -0.1831] \\
			  1 &  [-0.3879 -0.2980 -0.2642 -0.1756] \\
			 34 &  [-0.3772 -0.2610 -0.3058 -0.1950] \\
			 14 &  [-0.3973 -0.3282 -0.2157 -0.1940] \\
			  8 &  [-0.3976 -0.3300 -0.2168 -0.1818] \\
			 10 &  [-0.3905 -0.3282 -0.2347 -0.1782] \\
			 17 &  [-0.3686 -0.2827 -0.2844 -0.2055] \\
			 33 &  [-0.3719 -0.2382 -0.3301 -0.1956] \\
			 22 &  [-0.4088 -0.3750 -0.1544 -0.2217] \\
			  8 &  [-0.3920 -0.3329 -0.1997 -0.2457] \\
			 27 &  [-0.3717 -0.3144 -0.2299 -0.2456] \\
			 32 &  [-0.3729 -0.3083 -0.2337 -0.2462] \\
			 10 &  [-0.3650 -0.4040 -0.1959 -0.1712] \\
			  9 &  [-0.4346 -0.3141 -0.1937 -0.2169] \\
			  4 &  [-0.4195 -0.2945 -0.2243 -0.2181] \\
			 13 &  [-0.4073 -0.2735 -0.2562 -0.2169] \\
			 39 &  [-0.4143 -0.3073 -0.2150 -0.2018] \\
			  6 &  [-0.4038 -0.2954 -0.2414 -0.2007] \\
			 55 &  [-0.4016 -0.3023 -0.2326 -0.1978] \\
			  6 &  [-0.4004 -0.3001 -0.2374 -0.1880] \\
			 18 &  [-0.4056 -0.2675 -0.2683 -0.1782] \\
			  2 &  [-0.4108 -0.2348 -0.2992 -0.1683] \\
			  1 &  [-0.4174 -0.2629 -0.2621 -0.1584] \\
			 25 &  [-0.4173 -0.2566 -0.2685 -0.1508] \\
			  8 &  [-0.4099 -0.2943 -0.2523 -0.1350] \\
			 26 &  [-0.4047 -0.2917 -0.2607 -0.1254] \\
			  7 &  [-0.4080 -0.2472 -0.3065 -0.1155] \\
			  3 &  [-0.3935 -0.2111 -0.3470 -0.1315] \\
			  7 &  [-0.4095 -0.2150 -0.3308 -0.1124] \\
			  4 &  [-0.4355 -0.3085 -0.2329 -0.1152] \\
			 21 &  [-0.4281 -0.2599 -0.2809 -0.1719] \\
			 11 &  [-0.4203 -0.3556 -0.2430 -0.0968] \\
			  5 &  [-0.4304 -0.3256 -0.2547 -0.1130] \\
			  2 &  [-0.4244 -0.3156 -0.2768 -0.1120] \\
			 15 &  [-0.4070 -0.2761 -0.3242 -0.1371] \\
			  2 &  [-0.3831 -0.2305 -0.3741 -0.1660] \\
			 18 &  [-0.4115 -0.2968 -0.2692 -0.1703] \\
			  8 &  [-0.3966 -0.2625 -0.3095 -0.1845] \\
			 11 &  [-0.3834 -0.2435 -0.3427 -0.1839] \\
			 31 &  [-0.3942 -0.3066 -0.2891 -0.1758] \\
			 12 &  [-0.3939 -0.3171 -0.2751 -0.1681] \\
			  6 &  [-0.4021 -0.2875 -0.2879 -0.1827] \\
			 23 &  [-0.4039 -0.2993 -0.2741 -0.1754] \\
			  3 &  [-0.3860 -0.2515 -0.3255 -0.2078] \\
			 22 &  [-0.3595 -0.3529 -0.3122 -0.1530] \\
			 18 &  [-0.3732 -0.3511 -0.3031 -0.1372] \\
			  2 &  [-0.3697 -0.3498 -0.3060 -0.1358] \\
			  1 &  [-0.4112 -0.2786 -0.3068 -0.1999] \\
			  1 &  [-0.3953 -0.2565 -0.3380 -0.2011] \\
			 12 &  [-0.3851 -0.2455 -0.3610 -0.1952] \\
			 66 &  [-0.4319 -0.3021 -0.2513 -0.1985] \\
			 38 &  [-0.4263 -0.3052 -0.2495 -0.1941] \\
			  6 &  [-0.4142 -0.2645 -0.2971 -0.2202] \\
			 20 &  [-0.4007 -0.2539 -0.3206 -0.2162] \\
			 26 &  [-0.4357 -0.3795 -0.1690 -0.2237] \\
			  9 &  [-0.4532 -0.3314 -0.2511 -0.2031] \\
			  4 &  [-0.4330 -0.3135 -0.2807 -0.2010] \\
			  4 &  [-0.4172 -0.3048 -0.3038 -0.1927] \\
		\end{longtable}}
		
	Training error = 0.047018348623853214, test error = 0.06


	\item~[16 points] Implement the average Perceptron. Set the maximum number of epochs $T$ to 10. Report your learned weight vector. Comparing with the list of weight vectors from (b), what can you observe? Report the average prediction error on the test data. 

	% 2c
	\textit{Answer:}

	Averaged Perceptron model:

	Learned weight vector (sum of all weights investigated, a) = [-3272.47179459 -2221.1303575  -2052.70315333 -1251.99558238]

	Training error = 0.045871559633027525, test error = 0.06

	Comparing with the list shown in Table \ref{tab:2b} we see that the sum of all the investigated weights is much bigger than any single weight vector, this is because when we sum for averaged Perceptron we are not multiplying by $r$ like we do when just updating the weight vector, so we accumulate the weight vector. 
	
	Looking at the sum over the rows of Table \ref{tab:2b} (each learned weight vector) we get [-213.24239099, -148.8610516 , -135.24742741,  -85.95310053] which although different than the learned averaged Perceptron model the values are roughly the same in relative magnitude.


	\item~[14 points] Compare the average prediction errors for the three methods. What do you conclude? 

	% 2d
	\textit{Answer:}
	Comparing the prediction errors as seen in Table \ref{tab:2d} we can see that voted and averaged Perceptron perform very similarly. 
	The standard Perceptron algorithm is generally worse but I will not on certain runs it could perform better than voted and averaged Perceptron, depending on the data shuffle. 
	Also note that the standard Perceptron seemed to have its performance more affected by the random data shuffle than the voted and averaged Perceptron, which makes sense because of how voting and averaging helps reduce this type of variance.

	Overall, averaged Perceptron seems to be the best choice out of the Perceptron algorithms because of how it is simpler, and stores less residual data than the voted Perceptron and performs similarly than the voted Perceptron and is more robust than the standard Perceptron.

	\begin{center}
	\begin{longtable}{|c|c|c|}
		\caption{Prediction Errors by Model}
		\label{tab:2d}\\
		\hline
		Model & Train Prediction Error & Test Prediction Error \\
		\hline
		Perceptron & 0.0573 & 0.068 \\
		\hline
		Voted Perceptron & 0.0470 & 0.06 \\
		\hline
		Averaged Perceptron & 0.0459 & 0.06 \\ 
		\hline
	\end{longtable}
	\end{center}

\end{enumerate}


\end{enumerate}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
